{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3gLmEGEA3BJKjpx6webxG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nyq36GGJ-Hgn"},"source":["**This jupyer file together with two other jupyer files (the three files are: dataset-builder, utils, and tester) contain the code for redoing the tests conducted in the following paper:**\n","\n","\n","> Nazari, Ehsan and  Branco, Paula \"On Oversampling via Generative Adversarial Networks under Different Data Difficult Factors \" International Workshop on Learning with Imbalanced Domains: Theory and Applications. PMLR, 2021."]},{"cell_type":"markdown","metadata":{"id":"_6PCxhD77oaT"},"source":["This ipynb file contains the code for building a CGAN, and the needed functions for conducting our tests. \n","The code for building CGAN is obtained from: https://github.com/eriklindernoren/Keras-GAN/tree/master/cgan "]},{"cell_type":"code","metadata":{"id":"E3sney9wVsm6"},"source":["from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n","from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","import os\n","from math import sqrt\n","\n","class CGANRAW():\n","    def __init__(self,FEATURES, dominant_class_count, imbalance_rate):\n","        self.features_squared = int(sqrt(FEATURES))\n","        if(self.features_squared != sqrt(FEATURES)):\n","          raise Exception(\"number of features must be a whole number\") \n","        self.img_shape = (FEATURES,1)\n","        self.num_classes = 2\n","        self.latent_dim = 50\n","        self.classes_list = [0,1]\n","        self.name = 'class0_quantity_of_'+str(dominant_class_count)+'_with_imbalance_rate_of_'+str(imbalance_rate)+'_images'\n","        os.makedirs(self.name, exist_ok=True)\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss=['binary_crossentropy'],\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise and the target label as input\n","        # and generates the corresponding digit of that label\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,))\n","        img = self.generator([noise, label])\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated image as input and determines validity\n","        # and the label of that image\n","        valid = self.discriminator([img, label])\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains generator to fool discriminator\n","        self.combined = Model([noise, label], valid)\n","        self.combined.compile(loss=['binary_crossentropy'],\n","            optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(256, input_dim=self.latent_dim))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(1024))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n","        model.add(Reshape(self.img_shape))\n","\n","        # model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,), dtype='int32')\n","        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n","\n","        model_input = multiply([noise, label_embedding])\n","        img = model(model_input)\n","\n","        return Model([noise, label], img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.4))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.4))\n","        model.add(Dense(1, activation='sigmoid'))\n","        # model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        label = Input(shape=(1,), dtype='int32')\n","\n","        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n","        flat_img = Flatten()(img)\n","\n","        model_input = multiply([flat_img, label_embedding])\n","\n","        validity = model(model_input)\n","\n","        return Model([img, label], validity)\n","\n","    def train(self, epochs,X_train,y_train, batch_size=32):\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half batch of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs, labels = X_train[idx], y_train[idx]\n","\n","            # Sample noise as generator input\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","            # Generate a half batch of new images\n","            gen_imgs = self.generator.predict([noise, labels])\n","\n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n","            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Condition on labels\n","            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n","\n","            # Train the generator\n","            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n","\n","            # Plot the progress\n","\n","            # If at save interval => save generated image samples\n","            # if epoch % 20 == 0:\n","            #   self.sample_images('{}_{:05d}'.format(self.name, epoch))\n","              # print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","\n","    def sample_images(self, epoch):\n","        r, c = 2, 5 #do not change\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        sampled_labels0 = np.zeros(5).reshape(-1, 1)\n","        sampled_labels1 = np.ones(5).reshape(-1, 1)\n","        sampled_labels = np.concatenate((sampled_labels0,sampled_labels1))\n","\n","        gen_imgs = self.generator.predict([noise, sampled_labels])\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt,:,:].reshape(self.features_squared,self.features_squared), cmap='gray')\n","                axs[i,j].set_title(\"class %d\" % sampled_labels[cnt])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"{}/{}.png\".format(self.name,epoch))\n","        plt.close()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ss-RtaCe-eUm"},"source":["Imbalacing the given dataset:"]},{"cell_type":"code","metadata":{"id":"QT4yp4y2cgJV"},"source":["from sklearn.utils import shuffle\n","\n","def imbalancer(data, label, dominant_class_count, imbalance_rate):\n","  if(dominant_class_count>10000):\n","    raise Exception(\"Maximum number of samples for class0 is 10,000\") \n","  x_train_class0 = data[label == 0]\n","  x_train_class0 = x_train_class0[0:dominant_class_count]\n","  x_train_class1 = data[label == 1]\n","  class1_count = int(dominant_class_count*imbalance_rate)\n","  x_train_class1 = x_train_class1[0:class1_count]\n","\n","  data = np.concatenate((x_train_class0,x_train_class1))\n","  label = np.concatenate( (np.zeros(x_train_class0.shape[0]), np.ones(x_train_class1.shape[0]) ) ) \n","  \n","  return shuffle(data, label, random_state=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJU4l67m_B3C"},"source":["balancing the dataset by augmenting samples for the second class via given cgan:"]},{"cell_type":"code","metadata":{"id":"0PnA-v2Y_SCP"},"source":["def balancer(generator, dominant_class_count, imbalance_rate,\n","             epochs, train_x, train_y):\n","  gan = generator(train_x.shape[-1], dominant_class_count, imbalance_rate)\n","  gan.train(epochs, train_x, train_y)\n","  \n","  class0_count = np.count_nonzero(train_y == 0)\n","  class1_count = np.count_nonzero(train_y == 1)\n","  if class0_count < class1_count:\n","    raise Exception(\"class 1 has more samples than class zero! (should be vice versa)\") \n","  how_many_class1_samples_should_be_generated = class0_count - class1_count\n","\n","  \n","  labels = np.ones(how_many_class1_samples_should_be_generated).reshape(-1,1)\n","  noise = np.random.normal(0, 1, (how_many_class1_samples_should_be_generated, gan.latent_dim))\n","\n","  gen_imgs = gan.generator.predict([noise, labels])\n","\n","  train_x = np.append(train_x, gen_imgs.reshape(-1,train_x.shape[-1],), axis=0) \n","  train_y = np.append(train_y, labels.reshape(-1,))\n","\n","  train_x, train_y = shuffle(train_x, train_y, random_state=0)\n","\n","  return train_x, train_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEMQMwqwRwZa"},"source":["def print_result(precision_class_0, recall_class_0, precision_class_1, recall_class_1):\n","  print('precision of class zero: ', precision_class_0)\n","  print('recall   of  class zero: ', recall_class_0)\n","  print('precision of  class one: ', precision_class_1)\n","  print('recall of   class   one: ', recall_class_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtBi8ZmMpcxw"},"source":["\n","def five_fold_train_gan_then_balance_then_train_classifier(data,\n","                                                           label,\n","                                                           dominant_class_count,\n","                                                           imbalance_rate,\n","                                                           classifier,\n","                                                           epochs):\n","  '''\n","  a balanced dataset with two classes, each of which having 10,000 samples must be inputted\n","\n","  if the value of data_features, the sixth parameter is set to false,\n","  the classifier will be trained with the given dataset untouched.\n","  '''\n","\n","  data, label = imbalancer(data, label, dominant_class_count, imbalance_rate)\n","\n","  c0 = np.count_nonzero(label == 0)\n","  c1 = np.count_nonzero(label == 1)\n","\n","  from sklearn.model_selection import KFold\n","  kf = KFold(n_splits=5, random_state=0, shuffle=True)\n","  precision_0, recall_0,f1_0, precision_1, recall_1,f1_1 = 0,0,0,0,0,0\n","  for train_index, test_index in kf.split(data):\n","    X_train, X_test = data[train_index], data[test_index]\n","    y_train, y_test = label[train_index], label[test_index]\n","    #data augmentation:\n","    #if the dataset is not balanced:\n","    if c0 != c1:\n","      X_train, y_train = balancer(CGANRAW,dominant_class_count,\n","                                  imbalance_rate, epochs, X_train, y_train)\n","    #data augmentation:\n","    p0, r0, f0,p1, r1,f1 = classifier(X_train,y_train, X_test, y_test)\n","    precision_0 +=p0\n","    recall_0 +=r0\n","    f1_0 +=f0\n","    precision_1 +=p1\n","    recall_1 +=r1\n","    f1_1 +=f1\n","\n","  precision_0 /=5\n","  recall_0 /=5\n","  f1_0 /=5\n","  precision_1 /=5\n","  recall_1 /=5\n","  f1_1 /=5\n","\n","  print_result(precision_0, recall_0, precision_1, recall_1)\n","  return precision_0, recall_0, f1_0, precision_1, recall_1, f1_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXh4PZJdIkTM"},"source":["def five_fold_train_classifier(data,\n","                              label,\n","                              dominant_class_count,\n","                              imbalance_rate,\n","                              classifier):\n","  '''\n","  a balanced dataset with two classes, each of which having 10,000 samples must be inputted\n","  '''\n","\n","  data, label = imbalancer(data, label, dominant_class_count, imbalance_rate)\n","\n","  from sklearn.model_selection import KFold\n","  kf = KFold(n_splits=5, random_state=0, shuffle=True)\n","  precision_0, recall_0,f1_0, precision_1, recall_1,f1_1 = 0,0,0,0,0,0\n","  for train_index, test_index in kf.split(data):\n","    X_train, X_test = data[train_index], data[test_index]\n","    y_train, y_test = label[train_index], label[test_index]\n","    p0, r0, f0,p1, r1,f1 = classifier(X_train,y_train, X_test, y_test)\n","    precision_0 +=p0\n","    recall_0 +=r0\n","    f1_0 +=f0\n","    precision_1 +=p1\n","    recall_1 +=r1\n","    f1_1 +=f1\n","\n","  precision_0 /=5\n","  recall_0 /=5\n","  f1_0 /=5\n","  precision_1 /=5\n","  recall_1 /=5\n","  f1_1 /=5\n","\n","  print_result(precision_0, recall_0, precision_1, recall_1)\n","  return precision_0, recall_0, f1_0, precision_1, recall_1, f1_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPcPQNd-h-NA"},"source":["import matplotlib.pyplot as plt\n","import os\n","def save_imgs(data, label,name):\n","  os.makedirs('sample_images_'+name, exist_ok=True)\n","  r, c = 2, 5 #do not change\n","  sampled_labels0 = np.zeros(5).reshape(-1, 1)\n","  sampled_labels1 = np.ones(5).reshape(-1, 1)\n","  sampled_labels = np.concatenate((sampled_labels0,sampled_labels1))\n","\n","  class0 = data[label == 0]\n","  class1 = data[label == 1]\n","\n","  gen_imgs = np.concatenate((class0[0:5],class1[0:5]))\n","\n","  # Rescale images 0 - 1\n","  gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","  fig, axs = plt.subplots(r, c)\n","  cnt = 0\n","  for i in range(r):\n","      for j in range(c):\n","          axs[i,j].imshow(gen_imgs[cnt,:,:], cmap='gray')\n","          axs[i,j].set_title(\"class %d\" % sampled_labels[cnt])\n","          axs[i,j].axis('off')\n","          cnt += 1\n","  fig.savefig(\"{}/1.png\".format('sample_images_'+name))\n","  plt.close()"],"execution_count":null,"outputs":[]}]}